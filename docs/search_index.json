[["index.html", "Nhật Ký Data Lời nói đầu", " Nhật Ký Data Lê Huỳnh Đức 2024-06-10 Lời nói đầu "],["tháng-01-2024.html", "Chương 1 Tháng 01 2024 Ngày 15 Ngày 16 Ngày 17 Ngày 18", " Chương 1 Tháng 01 2024 Ngày 15 Hôm nay mình đã đọc gì 1.0.1 Efficient Feature Selection via Genetic Algorithms Đây là một bài viết trên Medium nói về việc sử dụng Generic Algorithms. Generic Algorithms Tiếng Việt được gọi là Giải thuật di truyền, là thuật toán đầu tiên mình học được ngay khi bắt đầu học môn Máy học thống kê ở trường. Mình đã áp dụng nó vào bài toán tìm nghiệm xấp xỉ của một phương trình đa thức. Về Giải thuật di truyền có các định nghĩa chính: Population (dân số) Xác suất đột biến Xác suất lai chéo Chiến lược chọn các phần tử con Để hiểu rõ hơn các bạn có thể tham khảo ở Wiki (https://en.wikipedia.org/wiki/Genetic_algorithm) Tóm gọn lại bài viết này xem danh sách N features là một chuỗi gen có độ dài là N và chứa các giá trị 1 nghĩa là feature tại vị trí i sẽ được chọn và bằng 0 nghĩa là feature tại vị trí i sẽ không được chọn Thuật toán sẽ khởi tạo Một quần thể gồm 8 chuỗi gen khác nhau và tiến hành đột biến hoặc lai chéo giữa chúng để ra chuỗi gen mới -&gt; tập Features đã được chọn. Sau đó sẽ dùng phương pháp đánh giá. Bài này sử dụng code dựa trên thư viện https://github.com/DEAP/deap Và code đầy đủ có tại https://github.com/FlorinAndrei/fast_feature_selection 1.0.2 Data Envelopment Analysis Hôm nay mình đã đọc Chương một của cuốn sách Data Envelopment Analysis Của tác giả William W. Cooper , Lawrence M. Seiford , Kaoru Tone. Cuốn sách này tuy xuất bản năm 2007 nhưng khá có ích đối với việc mình đang làm là đánh giá chất lượng. Đánh giá Hiệu suất là gì​? Khi chúng ta có đầu vào, và nhận được kết quả, chúng ta hay đánh giá hiệu suất của kết quả xem với đầu vào như thế thì kết quả liệu có tốt không.​ Một số ví dụ cơ bản: Giá thành/sản phẩm, Lợi nhuận/sản phẩm.​ \\[ \\frac{Output}{Input} \\] Công thức trên được gọi là thước đo hiệu quả.​ Chúng ta có thể mở rộng công thức trên với Nhiều đầu vào (Inputs) và nhiều đầu ra (Outputs)​ Độ dốc của đường nối với mỗi điểm và gốc tọa độ tương ứng với Sales/ Employee ( hiệu quả).​ Đường có độ dốc cao nhất (nối với điểm B) được gọi là Efficient Frontier (Đường biên hiệu quả).​ Các điểm sẽ nằm cùng 1 phía so với đường thẳng, hoặc ở trên, hoặc ở dưới.​ Vì đường biên này bao bọc các điểm dữ liệu, nên phân tích này được gọi là Data Envelopment Analysis.​ Chúng ta có thể vẽ một đường thống kê hồi quy y=0.622x để ước lượng mối quan hệ tuyến tính giữa input và output.​ Đường hồi quy này đi qua chính giữa của tập dữ liệu, do đó chúng ta có thể xem các điểm ở phía trên đường là hiệu quả tốt và điểm dưới là chưa tốt, khoảng cách giữa điểm tới đường hồi quy chính ta mức độ hiệu quả/không hiệu quả​ Khoảng cách từ các điểm đến đường Efficient Frontier nêu lên độ lệch so với điểm tốt nhất​ Ngày 16 1.0.3 Chuẩn bị tài liệu cho khóa Data Analyst Tập trung vào các nội dung như Lambda function là gì, map, reduce,filter kết hợp cùng với lambda function Iterable và Iterator là gì Iterable là kiểu cấu trúc có thể dùng vòng lặp for được, thay vì dùng index mình có thể for phần tử trong cấu trúc Iterator là kiểu duyệt của các Iterable, để tạo iterator mình có thể áp dụng hàm iter(a). Để duyệt phần tử kế tiếp mình dùng next(). Nếu không còn phần tử nào sẽ trả về lỗi StopIteration Các kiểu Iterator như enumerate, zip Tiếp theo là List comprehension, dict comprehension Các kiểu collections mới Counter : dùng để đếm nhanh số lượng các phần tử Defaultdict: handle lỗi khi key không có trong dict, thường trả về giá trị mặc định tùy mình set Namedtuple: định nghĩa một tuple có cấu trúc, tên của từng phần tử 1.0.4 Step-by-Step Guide to Designing a User-Friendly Application Là một nhà khoa học dữ liệu, các kỹ năng thu thập, xử lý và phân tích dữ liệu cũng như đào tạo các mô hình phức tạp của bạn sẽ cung cấp những hiểu biết sâu sắc có giá trị giúp hướng dẫn các bên liên quan chính đưa ra quyết định đúng đắn. Tuy nhiên, bạn có thể thường xuyên phải đối mặt với những thách thức trong việc truyền đạt những kết quả này một cách hiệu quả. Việc dựa vào các tài liệu tĩnh có thể không thể hiện được toàn bộ tiềm năng của mô hình của bạn và thu hút sự quan tâm của người dùng doanh nghiệp. Không có gì có thể khó chịu hơn việc công việc của bạn không được tận dụng tối đa tiềm năng của nó. Đây là nơi một ứng dụng năng động, tương tác phát huy tác dụng. Nó biến công việc của bạn thành một công cụ sống động, mời các bên liên quan đến một môi trường khám phá, cộng tác và ra quyết định theo thời gian thực. Xác định mục tiêu ứng dụng Đầu tiên, điều quan trọng là phải xác định được câu hỏi “Tại sao phải tạo ra ứng dụng”, dưới đây là một số câu hỏi gợi ý Ứng dụng của bạn giải quyết vấn đề gì Nó tăng thêm giá trị cho người dùng như thế nào Những điểm chính rút ra từ ứng dụng là gì Mục tiêu App Người dùng muốn biết họ nên gọi cho ai từ một danh sách rộng lớn Giới thiệu khách hàng mới tiềm năng Người dùng muốn xác định ai có nguy cơ rời đi và chiến lược nào có thể giữ chân họ Đề xuất chiến lược giữ chân khách hàng Người dùng muốn đảm bảo mô hình đáng tin cậy của mô hình Chứng minh độ tin cậy và độ chính xác Xác định người dùng của bạn Ví dụ Sale manager: Không quá kỹ thuật Ưu tiên các sơ đồ thông thường và giải thích văn bản Cần hành động kịp thời hàng ngày Quan tâm đến hiệu suất của mô hình và khả năng giải thích Lập kế hoạch cho cấu trúc và tính năng của ứng dụng Ngày 17 1.0.5 Chữa bài Là một bài về dự đoán chất lượng nguồn nước gồm các thông số cơ bản như Location Temp pH EC Do Turb TN TP Và nhãn là Toc Phần này mình có đọc code của một thầy bên Hàn, thầy áp dùng fill outlier bằng IQR rất đơn giản loại bỏ các điểm nằm ngoài Q3 + 3 * IQR và Q1 - 3 * IQR. Lần này thầy dùng hệ số bằng 3 chứ không bằng 1 như mặc dịnh Sau đó thầy vẽ các phân phối của nó ra trước khi xử lý def show_hist_by_target(df, columns): sns.set(font_scale=1.0) sns.set_style(&#39;white&#39;) for column in columns: fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 3)) sns.boxplot(data=df, y=column, ax=axs[0]) sns.histplot(data=df, x=column, ax=axs[1], kde=True, bins=200, stat=&#39;frequency&#39;) plt.subplots_adjust(hspace=0.8,wspace=0.6) Tiếp theo thầy sử dụng ktest và normal test xem dữ liệu có bị skewness không for k in df.columns: test_stat, p_val = stats.kstest(df[k], &#39;norm&#39;) print(&quot;Test-statistics : {:.3f}, p-value : {:.3f}, skewness : {:.3f}&quot;.format(test_stat, p_val, df[k].skew())) # (Kolmogorov-Smirnov) kstest / shapiro for k in df.columns: test_stat, p_val = stats.kstest(df[k], &#39;norm&#39;) print(&quot;Test-statistics : {:.3f}, p-value : {:.3f}, skewness : {:.3f}&quot;.format(test_stat, p_val, df[k].skew())) # (Kolmogorov-Smirnov) kstest / shapiro Thầy dùng R2 score để tính, sử dụng 2 cách là dùng thư vienj sklearn và dùng công thức điều chỉnh ############################################################################################################## def adj_r2_score(y_true, y_pred, p=x_train_scaled.shape[1]): return 1-(1-r2_score(y_true, y_pred)) * (len(y_true)-1) / (len(y_true) - p - 1) Thầy dùng Scaler là Robust Scaler, trước đó đã test box cox nhưng vì có tồn tại dữ liệu âm nên không dùng box-cox. Giải pháp của mình là: Remove các dòng âm và bằng 0 -&gt; khoảng 500 dòng trên 300k dòng Dùng thuật toán yeo-johnson tương tự như box cox nhưng có thể áp dụng cho số âm Mình cũng hướng dẫn bạn ấy sử dụng KFold thay vì train_test_split vì train_test_split có khả năng bị bias vì không xác thực chéo trên toàn bộ dữ liệu( bị bias). Kết quả mình test thử train_test_split với nhiều seed khác nhau thì kết quả R2 không đồng đều. Ngày 18 1.0.6 Kết hợp ChatGPT vào Named Entity Recognition 1.0.6.1 Giới thiệu về Named Entity Recognition Named Entity Recogtition là bài toán nhận dạng thực thể, một số thực thể thông dùng như PERSON : Tên người LOCATION: Địa chỉ DATE: Ngày tháng năm ORG: Tên các tổ chức. Ngoài ra tùy vào yêu cầu của mỗi bài toán có thể có những Named Entity Recoginition. Bài toán trích xuất thông tin xe thì có thêm BRAND, MODEL, YEAR, CAR_TYPE …. Bài toán trích xuất thông tin từ các sản phẩm trên các sàn thương mại điện tử : BRAND, PRODUCT_NAME, SIZE, PRICE Dựa vào các thông tin được trích xuất đó, chúng ta có thể làm các bài toán như sau: - Lưu trữ dữ liệu : sau khi trích xuất thông tin về một item, chúng ta có thể lưu trữ các thông tin đó vào các bảng có cấu trúc hoặc lưu vào SQL - Các bài toán Recommendation: Dựa vào các thông tin trích xuất đó, chúng ta có thể đưa ra gợi ý các sản phẩm cho người dùng. Ví dụ như: - Người dùng đang xem các bài báo liên quan đến xe, chúng ta có thể trích xuất hãng xe, dòng xe từ bài báo mà người dùng đọc, sau đó gợi ý các sản phẩm xe cùng với hãng đó. - Product Matching: Trên các sàn thương mại điện tử, nhiều nhà cung cấp có thể cùng bán một sản phẩm, việc trích xuất thông tin của các sản phẩm có thể giúp gom các sản phẩm giống nhau về cùng một nhóm, qua đó có thể nắm bắt được giá cả, sức mua của sản phẩm đó. 1.0.6.2 ChatGPT Để có thể nhận diện được các thực thể, thông thường chúng ta sẽ huấn luyện các mô hình với dữ liệu huấn luyện được gán nhãn trước. Một số ví dụ về huấn luyện các mô hình https://medium.com/@mjghadge9007/building-your-own-custom-named-entity-recognition-ner-model-with-spacy-v3-a-step-by-step-guide-15c7dcb1c416 https://www.analyticsvidhya.com/blog/2022/06/how-to-train-an-ner-model-with-huggingface/ https://blog.futuresmart.ai/building-a-custom-ner-model-with-spacy-a-step-by-step-guide Ngoài ra chúng ta cũng có thể tận dụng sức mạnh của chatGPT dưới đây là một ví dụ về prompt để lấy brand và model xe messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Perform Name Entity Recognition task \\ to extract car brand and car model name from paragraphs&quot;}, {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Output format: a string v1:v2 where v1 is car brand and v2 is car model for each input.\\ If not found car brand then v1 = None. If not found car model then v2 = None&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: text} ], Kết quả trả ra text brand model 0 Toyota Land Cruise LC300 is a good and powerfu… Toyota Land Cruiser LC300 1 I am a big fan of Jeep Wrangle Rubicon, an off… Jeep Wrangler Rubicon 2 Black Friday, buy cars at great prices None None 1.0.7 Phân tích Shop Branch Bài toán của mình là tìm ra các vị trí A, B sao cho - Bắt đầu từ A: Profit của các chi nhánh tăng vọt - Sau khi qua B: Profit của các chi nhánh đi ngang Sau đó đưa ra các điểm Ti, Tj là lần lượt là thời gian đạt được điểm A và điểm B Dựa vào đó, phòng ban Kinh doanh sẽ biết được các chi nhánh nào đang hoạt động không hiệu quả để đưa ra các giải pháp phù hợp "],["tháng-02-2024.html", "Chương 2 Tháng 02 2024 Ngày 20 Ngày 21", " Chương 2 Tháng 02 2024 Ngày 20 Hôm nay mình đã đọc gì 2.0.1 Cách visualize counts và labels cho histogram dùng seaborn và numpy Thông thường, khi visualize mặc định histogram của seaborn sẽ không thể hiện tất cả các nhãn của bins và số lượng mỗi bin import numpy as np import seaborn as sns np.random.seed(42) x = np.random.uniform(size=200) sns.histplot(x, bins=10) Như trên sẽ rất khó theo dõi các giá trị và số lượng, do đó cần phải tìm được giá trị của các bin và số lượng. May mắn thay seaborn histogram tương tự numpy nên ta có thể dùng hàm np.histogram counts, bin_labels = np.histogram(x, bins=10) print(len(counts), counts) print(len(bin_labels), bin_labels) 10 [22 24 20 24 11 19 20 18 18 24] 11 [0.00552212 0.1036586 0.20179508 0.29993156 0.39806804 0.49620453 0.59434101 0.69247749 0.79061397 0.88875045 0.98688694] Các bạn có thể thấy với tham số bins đưa vào chỉ có 10 trong khi kết quả nhãn bin_labels lại có 11 giá trị, trong đó giá trị đầu là giá trị nhỏ nhất của x và giá trị cuối là giá trị cao nhất của x. Do đó khoảng đầu tiên của bin đầu tiên có giá trị nằm từ [0.00552212, 0.1036586) Để kiểm tra xem có đúng thể không, ví dụ kiểm tra bin đầu tiên có phải 22 phần tử không len(x[(x&gt;=bin_labels[0]) &amp; (x&lt;bin_labels[1])]) 22 Chính xác!! Dùng matplotlib để plot text import matplotlib.pyplot as plt counts, bin_labels = np.histogram(x, bins=10) ax = sns.histplot(x, bins=10) bin_heights = [p.get_height() if p.get_height() &gt; 0 else 0.1 for p in ax.patches] # Hiển thị giá trị count từng bin for i in range(len(bin_heights)): plt.text(ax.patches[i].get_x() + ax.patches[i].get_width() / 2, bin_heights[i], str(int(bin_heights[i])), fontsize=14, ha=&#39;center&#39;, va=&#39;bottom&#39;) mids = [rect.get_x() + rect.get_width() / 2 for rect in ax.patches] labels = [(round(i,2),round(j,2)) for i,j in zip(bin_labels[:-1], bin_labels[1:])] ax.set_xticks(mids, labels=labels, fontsize=14, rotation=90) Ngày 21 Nhận ra rằng violinplot có vẻ không phù hợp bằng nhiều histogram plot, vì violinplot chỉ show kde distribution chứ k show histogram plot, do đó không biết được chính xác bin nào đang cao nhất "],["tháng-03-2024.html", "Chương 3 Tháng 03 2024 Ngày 3 Ngày 4 Ngày 8", " Chương 3 Tháng 03 2024 Ngày 3 https://towardsdatascience.com/location-analytics-use-cases-that-every-data-scientist-should-know-740b708a2504 Làm sao để xây dựng chiến lược mở rộng chi nhánh tốt nhất https://predikdata.com/how-to-create-the-best-branch-expansion-strategy/ Location intelligence Location Intelligence là một kỹ thuật phân tích dữ liệu nâng cao. Nó sử dụng thông tin địa lý để khám phá các patterns, xu hướng, và mối quan hệ dựa trên dữ liệu. Kỹ thuật này khai tháng sức mạnh của thông tin địa lý để tìm insight từ dữ liệu. Bằng cách kết hợp giữa dữ liệu kinh doanh và dữ liệu không gian, Location Intelligence cung cấp những thông tin có giá trị giúp ra quyết định tốt hơn. Có ba layer trong LI Layer 1: Geographical Layer 2: Store locations Layer 3: Sales data 5 Lợi ích khi sử dụng LI Data-driven dicision-marking: Sử dụng dữ liệu location để đánh giá khả năng tiếp cận của một cửa hàng so với đối thủ cạnh tranh Enhanced customer targeting : Sủ dụng location và heatmap để hiểu hành vi khách hàng https://predikdata.com/heat-maps-for-real-estate-investments/ Cải thiện lợi thế cạnh tranh: Phân tích khả năng di chuyện và lượng người qua lại giữa 2 loại cửa hàng: https://predikdata.com/foot-traffic-in-pharmacies-cvs-pharmacy-vs-walgreens/ Tối ưu hóa chi phí Quản lý nhân sự hợp lý Cách sử dụng LI cho chiến lược mở rộng chi nhánh Xác định thị trường Sử dụng nhân khẩu học, sở thích của người tiêu dùng, hành vi mua hàng và nghiên cứu thị trường để xác định khách hàng lý tưởng của bạn. Thông tin vị trí giúp bạn tìm thấy những khu vực có lượng khách hàng tiềm năng tập trung cao, tập trung nỗ lực mở rộng vào những khu vực hứa hẹn nhất. Phân tích đối thủ Đánh giá bối cảnh cạnh tranh, xác định vị trí và thị phần của đối thủ cạnh tranh. Khám phá những nhu cầu chưa được đáp ứng hoặc những khoảng trống thị trường để xác định cơ hội chi nhánh mới tại thị trường địa phương và quốc tế. Đánh giá khả năng tiếp cận: Ước tính mức độ dễ dàng tiếp cận các địa điểm chi nhánh tiềm năng, bao gồm cơ sở hạ tầng giao thông, mô hình giao thông và tình trạng sẵn có của bãi đậu xe. Các trang web dễ dàng truy cập cho khách hàng và nhân viên góp phần tạo nên sự thành công của chi nhánh Tối ưu hóa chi phí Bất Động Sản (https://predikdata.com/heat-maps-for-real-estate-investments/) Xác định các khu vực có giá bất động sản, giá cho thuê và quy định địa phương tối ưu bằng cách sử dụng thông tin vị trí. Cân bằng chi phí với lượng người qua lại và khả năng hiển thị cao đảm bảo lợi tức đầu tư cao, giúp bạn giảm chi phí và tăng doanh thu. Đánh giá lực lượng lao động địa phương Phân tích thị trường lao động địa phương để đảm bảo có sẵn nhân viên lành nghề và đáng tin cậy. Thông tin vị trí cung cấp thông tin chi tiết về trình độ học vấn, bộ kỹ năng và mức lương trung bình, giúp bạn đưa ra quyết định nhân sự. Quản lý hiệu suất Theo dõi hiệu quả hoạt động của chi nhánh mới bằng các chỉ số hiệu suất chính (KPI) và các tiêu chuẩn đã được thiết lập. Xác định các chi nhánh hoạt động kém hiệu quả và các yếu tố góp phần vào hiệu quả hoạt động của chúng để đưa ra quyết định dựa trên dữ liệu về việc cải thiện hoặc đóng cửa. Việc tích hợp chiến lược tiếp thị của bạn, bao gồm cả phương tiện truyền thông xã hội, với thông tin vị trí cho phép bạn thâm nhập các thị trường mới, nhắm mục tiêu vào các doanh nghiệp nhỏ và thúc đẩy mở rộng quốc tế. Bằng cách khai thác thông tin vị trí để mở rộng thị trường, bạn có thể cung cấp các sản phẩm hoặc dịch vụ phù hợp, tăng sức hấp dẫn đối với khách hàng tiềm năng và đảm bảo thành công lâu dài. Mở rộng chi nhánh sử dụng phân tích dự đoán Các bước để xác định vị trí tốt nhất cho từng điểm bán hàng Bước 1: Thu thập dữ liệu Dữ liệu Demographics như: Tổng số dân, giới tính và tình trạng cư trú /Zip code Các yếu tố kinh thế như sales, giá nhà Thông tin địa lý, vị trí đối thủ cạnh tranh Bước 2: Dự đoán doanh số cửa hàng cấp Zip -&gt; ước tính doanh thu tiềm năng cho từng địa điểm Bước 3: Chọn cấp ZIP có doanh số cao nhất Bước 4: Sử dụng các mô hình để - Hiểu các yếu tố thúc đẩy doanh số bán hàng - Dự báo doanh thu, đồng thời nắm bắt được sức ảnh hưởng “ăn lẫn nhau” (cannibalization) - Dự đoán nhu cầu ở cấp độ sản phẩm / phân khúc - Tạo điểm tương đồng cho cửa hàng Bước 5: Xác định các vị trí có thể tối ưu hóa trông qua kết hợp ví trị các cửa hàng, chi phí, tiềm năng Lặp lại các vị trí lưu trữ có thể có ở cấp Zip Giảm thiểu cannibalization bằng regression model Nhận diện khu vực Zip đang có lợi nhuận cao Bước 6: Xây dựng mô hình hồi quy cấp cửa hàng https://predikdata.com/footfall-analytics-for-site-selection-strategies/ https://predikdata.com/foot-traffic-in-pharmacies-cvs-pharmacy-vs-walgreens/ Cannibalization Analysis (Tạm dịch phân tích ăn mòn) Phân tích ăn mòn nhằm mục đích xác định liệu một shop mới có thu hút khách hàng khỏi những shop hiện có hay không, hay có khả năng tác động tiêu cực đến doanh thu của doanh nghiệp không, điều này rât quan trong đối với các doanh nghiệp có nhiều chi nhánh khác nhau, giúp họ đưa ra quyết định sáng suốt về địa điểm mở chi nhánh mới, di dời hoặc đóng cửa chi nhánh. Vậy, với những bài toán nào có thể tận dụng Phân tích ăn mòn Việc đặt một tháp 5G mới sẽ làm giảm tình trạng rời bỏ dịch vụ ở khu vực này như thế nào? Trạm sạc EV mới sẽ có tác động lớn nhất ở đâu Nếu bắt đầu bán chocolate ở chi nhánh X thì nó ảnh hưởng như thế nào đến doanh số bán hàng ở chi nhánh Y Phân tích ăn mòn có bốn bước chính Thu thập dữ liệu Xử lý dữ liệu Chạy phân tích ăn mòn Ra quyết định Thu thập dữ liệu Dữ liệu cần thu thập bao gồm Vị trí các shop hiện có: Vị trí tiềm năng: Là các vị trí đặt shop có thể gây ra “ăn mòn” Tiếp theo, chúng ta cần dữ liệu về những gì có thể thúc đấy quá trình “ăn mòn”. Ví dụ Dân số Lượng khách hàng Phân chồng chéo giữa 2 phần bao phủ của 2 cửa hàng urbanity levels: mức độ đô thị -&gt; có thể thay bằng mật độ dân số Xử lý dữ liệu Xây dựng hàm processing Với mỗi địa điểm, đưa mật độ dân số vào Với mỗi địa điểm, tại một bán kính chỉ định, trả về vùng ảnh hưởng Chạy phân tích ăn mòn Xây dựng hàm “phân tích”, kết quả trả ra các thông số như sau: Perc_cov_cells : là phần trăm diện tích vùng ảnh hưởng của shop mới đối với shop hiện có Perc_cov: là phần trăm dân số trùng lặp, method: STRING indicates the method of trade area generation. Three options available: buffer, kring, isoline and custom. This method applies to all locations provided. https://academy.carto.com/advanced-spatial-analytics/spatial-analytics-for-bigquery/step-by-step-tutorials/store-cannibalization-quantifying-the-effect-of-opening-new-stores-on-your-existing-network https://locatium.ai/locaium-ai-blog-cannibalization-analysis/ https://targomo.medium.com/introducing-cannibalization-analysis-examine-the-impact-of-competitors-and-network-branches-on-190e08ae002 https://carto.com/blog/cannibalization-analysis-tutorial https://github.com/CarlitosDev/causalCannibalisation Ngày 4 10 lỗi sai trong data visualization và cách né lỗi sai Dấu hiệu của một biểu đồ trực quan kém Dữ liệu liên quan bị ẩn: Những thông tin cần thiết bị che khuất hoặc không được đánh dấu Biểu đồ quá tải: Bao gồm quá nhiều thông tin khiến biểu đồ khó đọc Sai lệch dữ liệu: Sử dụng tỉ lệ hoặc định dạng dữ liệu sai Mô tả dữ liệu sai: Mô tả dữ liệu không đúng với hình vẽ Hình ảnh khó hiểu: Thiết kế hình ảnh gây khó hiểu làm cho người đọc không nắm được thông tin Độ tương phản màu gây hiểu lầm Đưa quá nhiều thông tin không liên quan Một sai lầm phổ biến là đưa quá nhiều màu vào visualize. Ví dụ vẽ biểu đồ scatter plot về dân số và tốc độ tăng trưởng dân số của Việt Nam, với 63 tỉnh thành ta có 63 màu cho mỗi tỉnh, điều này gây ra vấn đề khó khăn khi nhận biết màu nào là của tỉnh thành nào. Theo nguyên tắc chung, thang màu chất lượng hoạt động tốt nhất khi có ba đến năm loại màu khác nhau. Do đó, thay vì để 63 màu cho 63 tỉnh thành, chúng ta nên phân chia màu theo 7 khu vực kinh tế (hoặc 3 miền) và để xác định từng tỉnh thành, chúng ta thêm các nhãn tỉnh thành vào biểu đồ. Dĩ nhiên chúng ta không cần phải thêm tất cả các nhãn của tất cả tỉnh thành vào biểu đồ mà chỉ cần thêm một vài nhãn tiêu biểu ứng với từng khu vực. Ngoài ra, chúng ta cần nên kèm theo dữ liệu dạng bảng với các thông tin đầy đủ để người đọc có thể truy cập toàn bộ dữ liệu. Đối với dạng barchart việc sử dụng quá nhiều màu sắc(màu cầu vồng) không thể hiện được mục đích của màu sắc trong biểu đồ này image Sử dụng thang màu không đơn điệu(non-monotonic) để mã hóa giá trị dữ liệu Không thiết kế màu cho người giảm thị lực màu sắc https://clauswilke.com/dataviz/color-pitfalls.html Tác động của lỗi Người dùng hiểu nhầm giá trị nào quan trọng hơn Mất thời gian để hiểu thông tin Cách giải quyết Sử dụng dải màu ngắn để hiển thị giá trị cao hơn giá trị khác (ví dụ đỏ -&gt; vàng) Cách đơn giản nhất là chuyển dài màu đó về mức xám xem màu bạn chọn có thể hiện sự khác biệt không Sử dụng tông màu nóng/lạnh để thể hiện giá trị đối nghịch (tiêu cực/tích cực) Ngày 8 https://medium.com/towards-data-science/modeling-variable-seasonal-features-with-the-fourier-transform-18c792102047 "],["tháng-04-2024.html", "Chương 4 Tháng 04 2024 Ngày 26", " Chương 4 Tháng 04 2024 Ngày 26 4.0.1 Practical Computer Simulations for Product Analysts https://towardsdatascience.com/practical-computer-simulations-for-product-analysts-90b5deb6a54e "],["tháng-05-2024.html", "Chương 5 Tháng 05 2024 Ngày 24", " Chương 5 Tháng 05 2024 Ngày 24 5.0.1 Feature Engineering for Machine Learning https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30 5.0.2 Aggregation Tạo các feature từ agg feature (hay groupby từ một cái gì đấy) trên các cột dữ liệu dạng số theo các hàm như :one: sum :two: average :three: min :four: max :five: percentile :six: standard deviation :seven: coefficient of variation Ví dụ muốn nhận biết được giao dịch gian lận (những giao dịch có giá trị nhỏ liên tiếp). Chúng ta có thể lấy trung bình giá trị năm lần giao dịch liên tiếp, nếu các giao dịch gian lận thì khả năng trung bình của nó sẽ nhỏ hơn các giao dịch không gian lận 5.0.3 Differences and Ratios Trong một số trường hợp, việc thay đổi pattern cũng là một tín hiệu để nhận biết gian lận, ví dụ như số lượng lần giao dịch trong ngày đó tăng lên đột ngột chẳng hạn. Thì lúc đó ta có thể tính tỉ lệ Trong ví dụ trên, Mặc dù số lần giao dịch của S1 và S3, S8 là cao nhưng so với trung vị số lần giao dịch/ngày trong 30 ngày thì độ lệch thấp (1.05 và 0.8 và 1.17) Trong khi với S5 và S7 thì trung vị chỉ có 2 và 6 trong khi số lần giao dịch trong ngày là 16 và 27 gấp 8 lần và 4.5 lần so với thông thường "],["tháng-06-2024.html", "Chương 6 Tháng 06 2024 Ngày 07", " Chương 6 Tháng 06 2024 Ngày 07 Measuring The Intrinsic Causal Influence Of Your Marketing Campaigns https://towardsdatascience.com/measuring-the-intrinsic-causal-influence-of-your-marketing-campaigns-aa8354c26b7b Có cả notebook ở đây https://github.com/raz1470/causal_ai/tree/main/notebooks Bài này giúp trả lời những câu hỏi Những thách thức khi nói về việc đo lường tiếp thị Ảnh hưởng nhân quả nội tại là gì và cách thức hoạt động Những thách thức khi nói về việc đo lường tiếp thị Chiến dịch thương hiệu Mục đích của chiến dịch thương hiệu là để thu hút sự chú ý của những người mới. Những chiến dịch này thường chạy trên TV hoặc mạng xã hội và ở dạng video. Chúng thường không kêu gọi hành động trực tiếp như “our product will last you a lifetime”. Thách thức của việc quảng cáo trên TV là không xác định được ai đã xem quảng cáo. Điều này cũng xảy ra khi chúng ta xem quảng cáo trên Facebook và truy cập trang web mua hàng một cách tự nhiên mà không thông qua đường dẫn trên Facebook vào ngày hôm sau thì việc đo lường cũng gây khó khăn vì chúng ta không có cách liên kết hai sự kiện này NGoài ra còn có thách thức thứ hai là việc hiệu ứng vị delayed. Khi quảng cáo, có thế mất vài ngày/tuần/tháng để họ cân nhắc mua sản phẩm của bạn Chiến dịch hiệu suất Chiến dịch hiệu suất nhắm vào các khách hàng đang cân nhắc mua sản phẩm của bạn. Các chiến dịch này chạy trên các nền tảng tìm kiếm,mạng xã hội trả phí hoặc các kết tiếp thị liên kiết. Nó thường có dạng “click now to get 5% off your first purchase”. Với loại chiến dịch này thì xảy ra hai hình thức - Họ có thể nhấp vào và mua ngay nếu đã quen thương hiệu này - Họ có click nếu họ chưa biết đến brand? Nếu không cần chiến dịch quảng cáo thì có mua một cách tự nhiên không ( cụm organically hay dùng khi nói về các lượt tự nhiên ) -&gt; Cũng khó có thể đo lường Chiến dịch giữ chân Đơn giản là giữ chân khách hàng đã mua sản phẩm. Hay dùng A/B Testing Đồ thị lượt chuyển đổi Vì mỗi Node Demanh, TV Spend, Social spend đều có organic click và social clicks thì làm thế nào để tính được mỗi Node ảnh hưởng thế nào đến Revenue. Phương pháp đưa ra là dùng Nội Suy Causal Inference Ảnh hưởng nhân quả nội tại là gì và cách thức hoạt động Khái niệm này được đề cập trong paper năm 2020 https://proceedings.mlr.press/v238/janzing24a.html Và nó được triển khai trong module GCM trong package DoWhy https://www.pywhy.org/dowhy/v0.11.1/user_guide/causal_tasks/quantify_causal_influence/icc.html Nhắc lại Causal Graphs structural causal models (SCM) and additive noise models (ANM) A,B,C là các nốt gốc D, E không phải node gốc, chúng có thể tính D = A + B + C + noise E = A + B + C + noise F là nút đích vơi F = D + E + noise Rõ ràng là 3 nút gốc chỉ có noise Case Study Chọn nốt gốc nào để tối ưu Setup graph. node_lookup = {0: &#39;Demand&#39;, 1: &#39;TV spend&#39;, 2: &#39;Social spend&#39;, 3: &#39;Organic clicks&#39;, 4: &#39;Social clicks&#39;, 5: &#39;Revenue&#39; } total_nodes = len(node_lookup) # Create adjacency matrix - this is the base for our graph graph_actual = np.zeros((total_nodes, total_nodes)) # Create graph using expert domain knowledge graph_actual[0, 3] = 1.0 # Demand -&gt; Organic clicks graph_actual[0, 4] = 1.0 # Demand -&gt; Social clicks graph_actual[1, 3] = 1.0 # Brand spend -&gt; Organic clicks graph_actual[2, 3] = 1.0 # Social spend -&gt; Organic clicks graph_actual[1, 4] = 1.0 # Brand spend -&gt; Social clicks graph_actual[2, 4] = 1.0 # Social spend -&gt; Social clicks graph_actual[3, 5] = 1.0 # Organic clicks -&gt; Revenue graph_actual[4, 5] = 1.0 # Social clicks -&gt; Revenue Tạo dữ liệu # Create dataframe with 1 column per code df = pd.DataFrame(columns=node_lookup.values()) # Setup data generating process df[node_lookup[0]] = np.random.normal(100000, 25000, size=(20000)) # Demand df[node_lookup[1]] = np.random.normal(100000, 20000, size=(20000)) # Brand spend df[node_lookup[2]] = np.random.normal(100000, 25000, size=(20000)) # Social spend df[node_lookup[3]] = 0.75 * df[node_lookup[0]] \\ + 0.50 * df[node_lookup[1]] \\ + 0.25 * df[node_lookup[2]] \\ + np.random.normal(loc=0, scale=2000, size=20000) # Organic clicks df[node_lookup[4]] = 0.30 * df[node_lookup[0]] \\ + 0.50 * df[node_lookup[1]] \\ + 0.70 * df[node_lookup[2]] \\ + np.random.normal(100000, 25000, size=(20000)) # Social clicks df[node_lookup[5]] = df[node_lookup[3]] \\ + df[node_lookup[4]] \\ + np.random.normal(loc=0, scale=2000, size=20000) # Revenue Huấn luyện SCM import networkx as nx from dowhy import gcm # Setup graph graph = nx.from_numpy_array(graph_actual, create_using=nx.DiGraph) graph = nx.relabel_nodes(graph, node_lookup) # Create SCM causal_model = gcm.InvertibleStructuralCausalModel(graph) causal_model.set_causal_mechanism(&#39;Demand&#39;, gcm.EmpiricalDistribution()) # Deamnd causal_model.set_causal_mechanism(&#39;TV spend&#39;, gcm.EmpiricalDistribution()) # Brand spend causal_model.set_causal_mechanism(&#39;Social spend&#39;, gcm.EmpiricalDistribution()) # Social spend causal_model.set_causal_mechanism(&#39;Organic clicks&#39;, gcm.AdditiveNoiseModel(gcm.ml.create_ridge_regressor())) # Organic clicks causal_model.set_causal_mechanism(&#39;Social clicks&#39;, gcm.AdditiveNoiseModel(gcm.ml.create_ridge_regressor())) # Social clicks causal_model.set_causal_mechanism(&#39;Revenue&#39;, gcm.AdditiveNoiseModel(gcm.ml.create_ridge_regressor())) # Revenue gcm.fit(causal_model, df) Chúng ta có thể dùng auto_assignment để gán model tự động. Đánh giá mô hình ở đây https://www.pywhy.org/dowhy/v0.11.1/user_guide/modeling_gcm/model_evaluation.html Kết quả # calculate intrinsic causal influence ici = gcm.intrinsic_causal_influence(causal_model, target_node=&#39;Revenue&#39;) def convert_to_percentage(value_dictionary): total_absolute_sum = np.sum([abs(v) for v in value_dictionary.values()]) return {k: round(abs(v) / total_absolute_sum * 100, 1) for k, v in value_dictionary.items()} convert_to_percentage(ici) { &#39;Demand&#39;:0.31, &#39;TV spend&#39;:0.18, &#39;Social spend&#39;:0.23, &#39;Organic clicks&#39;:0.0, &#39;Social clicks&#39;: 0.28, &#39;Revenue&#39;:0.0 } … Khó hiểu là mấy trọng số 0.75, 0.5 , 0.3 ở đâu ra "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
